{
    "19": {
        "inputs": {
        "image": "20240730132115357147_model.png",
        "upload": "image"
        },
        "class_type": "LoadImage",
        "_meta": {
        "title": "Load Human Image"
        }
    },
    "21": {
        "inputs": {
        "image": "1234",
        "upload": "image"
        },
        "class_type": "LoadImage",
        "_meta": {
        "title": "Load Garment Image"
        }
    },
    "23": {
        "inputs": {
        "weight_dtype": "float16"
        },
        "class_type": "PipelineLoader",
        "_meta": {
        "title": "Load IDM-VTON Pipeline"
        }
    },
    "24": {
        "inputs": {
        "model_name": "GroundingDINO_SwinT_OGC (694MB)"
        },
        "class_type": "GroundingDinoModelLoader (segment anything)",
        "_meta": {
        "title": "GroundingDinoModelLoader (segment anything)"
        }
    },
    "25": {
        "inputs": {
        "model_name": "sam_vit_h (2.56GB)"
        },
        "class_type": "SAMModelLoader (segment anything)",
        "_meta": {
        "title": "SAMModelLoader (segment anything)"
        }
    },
    "26": {
        "inputs": {
        "prompt": "upper short sleeve",
        "threshold": 0.3,
        "sam_model": [
            "25",
            0
        ],
        "grounding_dino_model": [
            "24",
            0
        ],
        "image": [
            "19",
            0
        ]
        },
        "class_type": "GroundingDinoSAMSegment (segment anything)",
        "_meta": {
        "title": "GroundingDinoSAMSegment (segment anything)"
        }
    },
    "27": {
        "inputs": {
        "mask": [
            "26",
            1
        ]
        },
        "class_type": "MaskToImage",
        "_meta": {
        "title": "Convert Mask to Image"
        }
    },
    "28": {
        "inputs": {
        "garment_description": "",
        "negative_prompt": "",
        "width": 768,
        "height": 1024,
        "num_inference_steps": 30,
        "guidance_scale": 2,
        "strength": 1,
        "seed": 629363012808144,
        "pipeline": [
            "23",
            0
        ],
        "human_img": [
            "19",
            0
        ],
        "pose_img": [
            "34",
            0
        ],
        "mask_img": [
            "27",
            0
        ],
        "garment_img": [
            "21",
            0
        ]
        },
        "class_type": "IDM-VTON",
        "_meta": {
        "title": "Run IDM-VTON Inference"
        }
    },
    "29": {
        "inputs": {
        "images": [
            "27",
            0
        ]
        },
        "class_type": "PreviewImage",
        "_meta": {
        "title": "Preview Image"
        }
    },
    "30": {
        "inputs": {
        "images": [
            "34",
            0
        ]
        },
        "class_type": "PreviewImage",
        "_meta": {
        "title": "Preview Image"
        }
    },
    "31": {
        "inputs": {
        "images": [
            "28",
            0
        ]
        },
        "class_type": "PreviewImage",
        "_meta": {
        "title": "Preview Image"
        }
    },
    "32": {
        "inputs": {
        "filename_prefix": "ComfyUI",
        "images": [
            "28",
            0
        ]
        },
        "class_type": "SaveImage",
        "_meta": {
        "title": "Save Image"
        }
    },
    "34": {
        "inputs": {
        "model": "densepose_r50_fpn_dl.torchscript",
        "cmap": "Viridis (MagicAnimate)",
        "resolution": 512,
        "image": [
            "19",
            0
        ]
        },
        "class_type": "DensePosePreprocessor",
        "_meta": {
        "title": "DensePose Estimator"
        }
    }
    }